{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d08a33",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066061d5",
   "metadata": {},
   "source": [
    "A neural network is inspired by the structure of the human brain. Its basic building block is a neuron, which takes inputs, performs computations, and produces an output. Each neuron has an associated activation function that determines whether it should \"fire\" based on its inputs.  \n",
    "\n",
    "The training of a neural network can basically be broken down into the following parts:  \n",
    "#### 1. Architecture and Layers\n",
    "A simple neural network consists of an input layer, a hidden layer and an output layer. For deep neural networks, the hidden layers should be more than one and possibly sometimes the number of input features are also larger.\n",
    "\n",
    "##### Simple Neural Network\n",
    "$x_{1}$, $x_{2}$ and $x_{3}$ are the input features and $y_{1}$ and $y_{2}$ are the outputs. Here, we can say, the model architecture is receiving 3 features and 2 outputs(2 classes) in a classification model.\n",
    "<img src='img/nn.png' alt='Simple Neural Network' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "#### 2. Parameters\n",
    "In between each layers, there are some linear functions which consist of trainable parameters called weights and biases. Basically, the training of a neural network is updating these weights and biases to achieve the objective function of the specific problem.\n",
    "\n",
    "##### Linear function with parameters\n",
    "Let's assume that $x$ is an input feature from a data, the linear function is such a way that the weight $w$ multiple with the input feature $x$ and sum up with the bias $b$. In below graph, we explicitly assign $w=1/2$ and $b=1$ for easy understanding, and it shows a linear line representing that function. This example is only happening in one neuron or one feature. However in neural networks, there will be a lot of similar operations happening in parallel. In such cases, the vectorization and matrix operations are used for faster and more efficient computations.  \n",
    "<img src='img/linear.png' alt='Linear Function' width=\"250px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "\n",
    "#### 3. Activation Function\n",
    "The activation function introduces non-linearity to the network. It decides whether a neuron should be active or not based on its weighted inputs. Common activation functions include sigmoid, tanh, and Rectified Linear Unit (ReLU).\n",
    "\n",
    "<img src='img/activation.png' alt='Activation' width=\"250px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Deep Neural Network\n",
    "Architecture image referenced from: [Industry 4.0 Interoperability, Analytics, Security, and Case Studies](https://www.researchgate.net/figure/a-Typical-Architecture-of-Deep-Learning-Neural-Network-with-One-Output-One-Input-and_fig1_355485828)\n",
    "<img src='img/deepNN.png' alt='Deep Neural Network' width=\"600px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "#### 4. Feedforward\n",
    "In the feedforward neural network, the calculation is only forward pass without updating any parameters yet. The idea is first we calculate the linear function using the input features, then again from the outputs of the linear function, an activation function is introduced in order to get the non-linear outputs. Then the activated outputs from the previous layer go to the next layer as the input of that layer. The process is generally repeated through all the layers until we get the final result $y_{i}$.\n",
    "\n",
    "#### 5. Loss Function\n",
    "A loss function measures how far off the neural network's output is from the expected output(label or ground truth). It quantifies the network's performance and guides the learning process.\n",
    "\n",
    "#### 6. Optimizer\n",
    "An optimizer is an algorithm or a method used to adjust the parameters (weights and biases) of a neural network during training in order to minimize the loss function or maximize the efficiency of the model. The optimizer helps in adjusting the learning rate, momentum and direction of the gradient calculation during training.\n",
    "Examples of optimizer:  \n",
    "\n",
    "1. Stochastic Gradient Descent(SGD)\n",
    "2. Adam\n",
    "3. RMSProp\n",
    "4. Adagrad\n",
    "and so on...  \n",
    "\n",
    "#### 7. Backpropagation\n",
    "Backpropagation is the process of updating the weights and biases in the network to minimize the loss function(objective function). It calculates the gradient of the loss with respect to the network's parameters and updates them using optimization algorithms like Gradient Descent.\n",
    "\n",
    "##### Computational Graph\n",
    "<img src='img/computationgraph1.png' alt='Computation Graph' width=\"250px\" style=\"float: left\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/computationgraph2.png' alt='Computation Graph' width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Derivation\n",
    "<img src='img/backprop1.png' alt='Back Propagation' width=\"300px\" style=\"float: left\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "<img src='img/backprop2.png' alt='Back Propagation' width=\"300px\" style=\"float: left\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Derivative Examples\n",
    "$f(x)=x^{2}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $2x$\n",
    "\n",
    "$f(x)=x^{3}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $3x^{2}$\n",
    "\n",
    "$f(x)=log_{e}(x)$ or $f(x)=ln(x)$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $\\frac{1}{x}$\n",
    "\n",
    "$f(x)=e^{x}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $e^{x}$\n",
    "\n",
    "$f(x)=2x^{2}+3x+b$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $4x+3$ (The derivative of a constant is always zero)\n",
    "\n",
    "#### 8. Training\n",
    "The network is trained using labeled data. During training, the input data is passed through the network, the output is compared to the expected output, and the loss is calculated. Backpropagation then adjusts the weights and biases to minimize the loss so that we will get the optimal weights and biases that fit with our problem.\n",
    "\n",
    "##### Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used in machine learning and deep learning to iteratively update the parameters of a model in order to minimize a loss function. The goal of the algorithm is to find the values of the model's parameters that result in the lowest possible value of the loss function, which in turn signifies the best fit of the model to the training data.  \n",
    "\n",
    "1. Initialization: The algorithm starts with an initial guess for the model's parameters.  \n",
    "\n",
    "2. Compute Loss: The current model's parameters are used to make predictions on the training data. The difference between these predictions and the actual target values (the loss) is calculated using a chosen loss function.  \n",
    "\n",
    "3. Compute Gradient: The gradient of the loss with respect to each parameter is computed. The gradient indicates the direction and magnitude of the steepest increase in the loss function. This step involves taking partial derivatives of the loss function with respect to each parameter.  \n",
    "\n",
    "4. Update Parameters: The parameters are updated by subtracting a fraction of the gradient from the current parameter values. This fraction is called the learning rate. The learning rate determines the step size in the parameter space and influences the speed and stability of convergence.  \n",
    "\n",
    "5. Repeat: Steps 2 to 4 are repeated for a certain number of iterations or until the change in the loss function becomes small (convergence criterion).  \n",
    "\n",
    "Gradient descent algorithm reference: Deep Learning Specialization by Andrew Ng from [Coursera](https://www.coursera.org/).  \n",
    "\n",
    "<img src='img/gradientdescent.png' alt='Gradient Descent Algorithm' width=\"300\" style=\"float: left\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/localoptima.png' alt='Local Optima' width=\"175\" style=\"float: right\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/gradientdescent2.png' alt='Gradient Descent' width=\"400\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "#### 9. Validation and Evaluation\n",
    "After training, the network's performance is evaluated using validation data to ensure it's not overfitting, underfitting, or the model is learning. Once satisfied, the network is tested on unseen data to measure its real-world performance. The testing process is sometimes called inferencing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f59675",
   "metadata": {},
   "source": [
    "## Vectorization and Matrix Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a139b7",
   "metadata": {},
   "source": [
    "Before you start, please make sure that you have already installed `Numpy` library which is a handy package for sientific computing in Python.  \n",
    "To install Numpy, just simply use `pip3 install numpy` or `pip install numpy` in your terminal, or put an exclaimation point infront `!pip3 install numpy` or `!pip install numpy` in you Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908db90",
   "metadata": {},
   "source": [
    "#### Different types of vector/matrix operations\n",
    "1. The dot product (inner scalar product)  \n",
    "$\\mathbf{x1} \\cdot \\mathbf{x2}$  \n",
    "<img src=\"img/dot.png\" alt=\"Dot Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "2. Elementwise product (Hadamard product)  \n",
    "$\\mathbf{x1} \\circ \\mathbf{x2}$  \n",
    "$\\mathbf{x1} \\odot \\mathbf{x2}$  \n",
    "<img src=\"img/hadamard.png\" alt=\"Hadamard Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "3. The outer product  \n",
    "$\\mathbf{x1} \\otimes \\mathbf{x2}$  \n",
    "<img src=\"img/outer.png\" alt=\"Outer Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f1871",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c06ee",
   "metadata": {},
   "source": [
    "$\\mathbf{A}$ = $\\begin{pmatrix} 2&1 \\\\ 6&5  \\end{pmatrix}$, \n",
    "$\\mathbf{B}$ = $\\begin{pmatrix} 3&4 \\\\ 8&7  \\end{pmatrix}$  \n",
    "\n",
    "Calculate the following and print out the results:  \n",
    "1. $\\mathbf{A} \\cdot \\mathbf{B}$.\n",
    "2. $2\\mathbf{A}$\n",
    "3. $2\\mathbf{A}^{T}$\n",
    "4. $3\\mathbf{A}^{T} \\cdot \\mathbf{B}$\n",
    "5. $\\mathbf{A} \\odot \\mathbf{B}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2eb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing libraries before writing your codes\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "# np.random.seed(27)\n",
    "# x1 = np.random.randint(500, 1000, 500)\n",
    "# x2 = np.random.randint(0, 500, 500)\n",
    "# print(\"x1[0]:\", x1[0], \"  \", \"x2[0]:\", x2[0])\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"dot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"outer ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"elementwise multiplication ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"gdot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb63af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "# np.random.seed(27)\n",
    "# x1 = np.random.randint(500, 1000, 500)\n",
    "# x2 = np.random.randint(0, 500, 500)\n",
    "# print(\"x1[0]:\", x1[0], \"  \", \"x2[0]:\", x2[0])\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"dot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"outer ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"elementwise multiplication ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"gdot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23677d2",
   "metadata": {},
   "source": [
    "## Building basic functions with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efbaa1",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Referenced from [Action functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990ae47",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "Sigmoid function is known as the logistic function. The equation can be written as: \n",
    "\n",
    "$$\\textit{sigmoid(x)}=\\frac{1}{1+e^{-\\textit{x}}}$$\n",
    "\n",
    "<img src='img/sigmoid.png' alt='Sigmoid Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83969681",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4470636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def math_sigmoid(x):\n",
    "    '''\n",
    "    Compute sigmoid of x\n",
    "    '''\n",
    "    z = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "print(math_sigmoid(5))\n",
    "\n",
    "assert math_sigmoid(10) > 0.9999, \"Calculate error\"\n",
    "assert math_sigmoid(-10) < 0.0001, \"Calculate error\"\n",
    "assert math_sigmoid(0) == 0.5, \"Calculate error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37c23",
   "metadata": {},
   "source": [
    "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18098dd4",
   "metadata": {},
   "source": [
    "In fact, if $\\mathit{x = (x_{1}, x_{2},...,x_{n})}$ is a row vector then will apply the exponential function to every element of $\\mathit{x}$. The output will thus be:  \n",
    "\n",
    "$$\\mathit{np.exp(x) = (e^{x_{1}},e^{x_{2}},...,e^{x_{n}})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d98a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61b854",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "# Use Numpy library to build your sigmoid function again\n",
    "\n",
    "def Sigmoid(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Sigmoid(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7109, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.5498, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.5250, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.4256, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.3318, \"sigmoid output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb00587",
   "metadata": {},
   "source": [
    "#### ReLU (Rectified Linear Unit)\n",
    "A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: $\\textit{max(0,z)}$\n",
    ". Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.  \n",
    "\n",
    "$$\\textit{ReLU(x)} = \\textit{max(0, x)}$$  \n",
    "\n",
    "<img src='img/relu.png' alt='ReLU Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd4f15",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def ReLu(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45246787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = ReLu(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"ReLu output is incorrect\"\n",
    "assert y_hat[3] > a[3] and y_hat[3] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[4] > a[4] and y_hat[4] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[0] == a[0], \"ReLu output is incorrect\"\n",
    "assert y_hat[1] == a[1], \"ReLu output is incorrect\"\n",
    "assert y_hat[2] == a[2], \"ReLu output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07138b1",
   "metadata": {},
   "source": [
    "#### Tanh (Hyperbolic Tangent)\n",
    "Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.  \n",
    "\n",
    "$$\\textit{Tanh(x)} = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$  \n",
    "\n",
    "<img src='img/tanh.png' alt='Tanh Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f62555",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Tanh(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Tanh(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7163, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.1974, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.0997, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == -0.2913, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == -0.6044, \"Tanh output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bfa05",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "Softmax function calculates the probabilities distribution of the event over ‘i’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.  \n",
    "\n",
    "$$Softmax(z_{i}) = \\frac{exp(z_{i})}{\\sum{exp(z_{j})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ed1bb",
   "metadata": {},
   "source": [
    "#### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax for each row of the input x.\n",
    "\n",
    "    The code should work for a row vector and also for matrices of shape (m,n).\n",
    "    \"\"\"\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccf10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Softmax(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.4083, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.2028, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.1835, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.1230, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.0824, \"Softmax output is incorrect\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
