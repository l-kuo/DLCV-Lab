{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-AlexNet and GoogLeNet\n",
    "#### You can read the paper in [here](https://arxiv.org/abs/1512.00567)\n",
    "In this lab, we will develop PyTorch implementations of [AlexNet Paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) and [GoogleLeNet Paper](https://arxiv.org/abs/1409.4842) from scratch and compare them on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-call Our Essential Terminology\n",
    "\n",
    "**Tensor** - any matrix arrays which use for calculation, you can call input, output, weight as any tensors. In CNNs, we use \"input tensor\" as input; i.e. image, and \"output tensor\" as output.\n",
    "\n",
    "**Kernel** - filter tensor, or weight tensor. In computer vision, we might call mask tensor or mask matrix.\n",
    "\n",
    "**Channel** - number of depth in tensor, so sometime we call **depth**.\n",
    "\n",
    "**Feature** - Specific characteristic information for using in **dense layers** or **fully connect layers**.\n",
    "\n",
    "**Feature extraction** - the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set.\n",
    "\n",
    "**Stride** - The jump necessary to go from one element to the next one in the specified dimension dim . A tuple of all strides is returned when no argument is passed in.\n",
    "\n",
    "**Padding** - the zero array extends in both sides of tensor.\n",
    "\n",
    "In PyTorch implementation, the function of CNN layer does not need the input size, instead it requires the number of channels and the kernel size. For dense layer or fully connected layer, we need to set the number of input features and the number of output features. Therefore, it is essential to understand how to calculate the output size when we pass the input through the convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run output tensors size\n",
    "\n",
    "If we have an input tensor or image input size $w \\times h$ which want to convolution with $k_w \\times k_h$ kernel size with padding $p$ and stride $s$, we can calculate output tensor size as:\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor \\times \\lfloor \\frac{h+2p-k_h}{s} + 1 \\rfloor $$\n",
    "\n",
    "For example, input image in the first layer is $224 \\times 224$. Using $11 \\times 11$ of kernel size with padding $2$ and stride 4. We calculate\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor = \\lfloor \\frac{224+2(2)-11}{4} + 1 \\rfloor = \\lfloor 55.25 \\rfloor = 55$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet Implementation\n",
    "\n",
    "From class, we now know that the AlexNet model (1 machine) looks like this:\n",
    "\n",
    "<img src=\"img/alexnet.png\" title=\"AlexNet (Alex et al.)\" style=\"width: 900px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding in PyTorch\n",
    "\n",
    "First, we import some necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataset\n",
    "\n",
    "Next, we set up Dataset objects and DataLoader objects to load images, transform them to 3x224x224, and batch them for training/testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    # Your root path in gdrive\n",
    "    root_path = 'gdrive/My Drive/'\n",
    "else:\n",
    "    root_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up preprocessing of CIFAR-10 images to 3x224x224 with normalization\n",
    "# using the magic ImageNet means and standard deviations. You can try\n",
    "# RandomCrop, RandomHorizontalFlip, etc. during training to obtain\n",
    "# slightly better generalization.\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233, 0.24348505, 0.26158768))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root_path + 'data', train=True,\n",
    "                                             download=True, transform=preprocess)\n",
    "\n",
    "# Split the training set into training and validation sets randomly.\n",
    "# CIFAR-10 train contains 50,000 examples, so let's split 80%/20%.\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "# Download the test set. If you use data augmentation transforms for the training set,\n",
    "# you'll want to use a different transformer here.\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root_path + 'data', train=False,\n",
    "                                            download=True, transform=preprocess)\n",
    "\n",
    "# Dataset objects are mainly designed for datasets that can't fit entirely into memory.\n",
    "# Dataset objects don't load examples into memory until their __getitem__() method is\n",
    "# called. For supervised learning datasets, __getitem__() normally returns a 2-tuple\n",
    "# on each call. To make a Dataset object like this useful, we use a DataLoader object\n",
    "# to optionally shuffle then batch the examples in each dataset. During training.\n",
    "# To keep our memory utilization small, we'll use 4 images per batch, but we could use\n",
    "# a much larger batch size on a dedicated GPU. To obtain optimal usage of the GPU, we\n",
    "# would like to load the examples for the next batch while the current batch is being\n",
    "# used for training. DataLoader handles this by spawining \"worker\" threads that proactively\n",
    "# fetch the next batch in the background, enabling parallel training on the GPU and data\n",
    "# loading/transforming/augmenting on the CPU. Here we use num_workers=2 (the default)\n",
    "# so that two batches are always ready or being prepared.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4,\n",
    "                                              shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"AlexNet\" Using the Module API\n",
    "\n",
    "The Sequential API makes it easy to create a sequential model, but not all models are sequential. For example, we need more flexibility\n",
    "to create a complex model such as GoogLeNet.\n",
    "\n",
    "Working with the `Module` API requires us to use object-oriented inheritance in Python. This means you'll have to brush up on your OO concepts or learn the basics if OOP is new to you.\n",
    "\n",
    "We create a new class that inherits from `Module`, then in most cases, we just need to override two methods: `__init__()` and `forward()`.\n",
    "\n",
    "`__init__()` is called the \"constructor\" of a class and is the method called on any Python object just after it is created, similar to constructors in Java or C++.\n",
    "\n",
    "However, constructors and instance methods work a little differently in Python than they do in Java or C++. The constructor is just an ordinary instance method that is only special in that it\n",
    "is called implicitly when the object is created. Instance methods in Python (methods called on an object) are distinguished from class methods (methods called on the class, not requiring any\n",
    "instance) by the presence or absence of the `self` keyword in the parameter list. In the body of an instance method, `self` is a reference to the instance the method was called on, same as\n",
    "`this` in Java or C++ or `self` in Ruby.\n",
    "\n",
    "Anther difference between Python and other languages is that object initialization in an inheritance hierarchy is more flexible.\n",
    "A constructor should normally call `super(ClassName, self).__init__()` (Python 2, also works in Python 3) or `super().__init__()` (Python 3 only) at the beginning of its\n",
    "own `__init__()` method to initialze any fields used by methods in the superclass, but it need not do so.\n",
    "\n",
    "In the case of a PyTorch `Module` subclass, we should call `super()` before doing other things.\n",
    "\n",
    "The `forward()` method is also an instance method that is implicitly called when we invoke a `Module` instance as a function. So the code\n",
    "\n",
    "    module = MyModule()\n",
    "    \n",
    "creates an instance of `MyModule` and then calls its `__init__()` method, whereas\n",
    "\n",
    "    outputs = module(inputs)\n",
    "\n",
    "invokes the `forward()` method defined in `MyModule`.\n",
    "\n",
    "### The model\n",
    "\n",
    "Here's an implementation of an AlexNet-like network.\n",
    "\n",
    "Note that `Sequential` is itself a subclass of `Module`. This means we can use `Sequential` for a sequential flow in a larger network.\n",
    "\n",
    "Also note that the adaptive average pool layer between the feature module and the classifier is a trick used to ensure a fixed set of 6x6 feature maps come out\n",
    "of the feature extractor regardless of the input image size. It is not strictly required here (and not used in the original paper) but would allow us to use other input sizes besides 224x224 if we like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModule(nn.Module):\n",
    "    '''\n",
    "    An AlexNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of classes in the final multinomial output layer\n",
    "    features : Sequential\n",
    "        The feature extraction portion of the network\n",
    "    avgpool : AdaptiveAvgPool2d\n",
    "        Convert the final feature layer to 6x6 feature maps by average pooling if they are not already 6x6\n",
    "    classifier : Sequential\n",
    "        Classify the feature maps into num_classes classes\n",
    "    '''\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up to execute on a particular GPU or the CPU only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda, or cuda:0 means using GPU slot 0.\n",
    "# If there are more than 1 GPUs, you can select other GPUs using cuda:1, cuda:2, ...\n",
    "# In terminal (Linux), you can check memory using in each GPU by using command\n",
    "# $ nvidia-smi\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the model from <code>AlexNetModule</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = AlexNetModule(10)\n",
    "alexnet = alexnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Next, let's write a function to train our model for some number of epochs. This one is adapted from the PyTorch tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    '''\n",
    "    train_model function\n",
    "\n",
    "    Train a PyTorch model for a given number of epochs.\n",
    "    \n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over the train/validation dataset according to which phase we're in\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                # Inputs is one batch of input images, and labels is a corresponding vector of integers\n",
    "                # labeling each image in the batch. First, we move these tensors to our target device.\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out any parameter gradients that have previously been calculated. Parameter\n",
    "                # gradients accumulate over as many backward() passes as we let them, so they need\n",
    "                # to be zeroed out after each optimizer step.\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Instruct PyTorch to track gradients only if this is the training phase, then run the\n",
    "                # forward propagation and optionally the backward propagation step for this iteration.\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # The inception model is a special case during training because it has an auxiliary\n",
    "                    # output used to encourage discriminative representations in the deeper feature maps.\n",
    "                    # We need to calculate loss for both outputs. Otherwise, we have a single output to\n",
    "                    # calculate the loss on.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Backpropagate only if in training phase\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Gather our summary statistics\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # If this is the best model on the validation set so far, deep copy it\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Output summary statistics, load the best weight set, and return results\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss function\n",
    "\n",
    "Before we start training, we need to set up an optimizer object and a loss function. Typical choices for the loss function:\n",
    "* For regression problems, use `nn.MSELoss()`. The equation is:\n",
    "$$MSE=\\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2 $$\n",
    "* For binary classification, use `nn.BCELoss()`\n",
    "$$BCE=-\\frac{1}{N}\\sum_{i=1}^N y_i\\cdot \\log{\\hat{y}_i} + (1-y_i)\\cdot \\log(1 -{\\hat{y}_i})$$\n",
    "* For multinomial classification, use `nn.CrossEntropyLoss()`\n",
    "$$CE=-\\sum_{i=1}^C t_i\\cdot \\log(f(s)_i)$$\n",
    "where $t_i$ and $s_i$ are the groundtruth and the CNN score for each class $i$ in $C$.  An activation function (Sigmoid / Softmax) is usually applied to the scores before the CE Loss computation, so $f(s)_i$ to refer to the activations.\n",
    "* For specialized needs, define your own loss function!\n",
    "\n",
    "Typical choices for the optimizer:\n",
    "* SGD: Scholastic gradient descent, works well for most cases but requires appropriate values for the learning rate, momentum, and weight decay. Given $\\alpha$ is learning rate, and $\\beta$ is momentum, the equation is\n",
    "\n",
    "$$V_t = \\beta V_{t-1} + (1-\\beta)\\nabla_wL(W,X,y)$$\n",
    "$$W_{t+1} = W_t + \\alpha V_t$$\n",
    "\n",
    " for more information please see [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
    "\n",
    "* Adam: adaptive learning rate optimizer that usually gives superior results to SGD but sometimes doesn't work. Adam's equation is:\n",
    "$$(m_t)_i=\\beta_1(m_{t-1})_i+(1-\\beta_1)(\\nabla(W_t))_i,$$\n",
    "$$(v_t)_i=\\beta_2(v_{t-1})_i+(1-\\beta_2)(\\nabla(W_t))_i^2,$$\n",
    "$$(W_{t+1})i=(W_t)_i-\\alpha\\frac{\\sqrt{1-(\\beta_2)_i^t}}{1-(\\beta_i)_i^t}\\frac{(m_t)_i}{\\sqrt{(v_t)_i}+\\epsilon}$$\n",
    "* See the many other choices selected from recent deep learning papers in the [PyTorch optim documentation](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CrossEntropyLoss for multinomial classification (Because we have 10 classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# parameters = weights\n",
    "params_to_update = alexnet.parameters()\n",
    "# Use scholastic gradient descent for update weights in model with learning rate 0.001 and momentum 0.9\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Use train_model function for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = { 'train': train_dataloader, 'val': val_dataloader }\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(alexnet, dataloaders, criterion, optimizer, 10, 'alex_sequential_lr_0.001_bestsofar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Based on these results, let's plot the validation loss/accuracy curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(val_acc_history, loss_acc_history):\n",
    "    plt.plot(loss_acc_history, label = 'Validation')\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(val_acc_history, label = 'Validation')\n",
    "    plt.title('Accuracy per epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(val_acc_history, loss_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleLeNet\n",
    "\n",
    "Next, we'd like to construct GoogLeNet as described in the [original GoogLeNet paper](https://arxiv.org/abs/1409.4842) from scratch.\n",
    "\n",
    "This part of the lab is adapted from [kuangliu's PyTorch CIFAR repository on GitHub](https://github.com/kuangliu/pytorch-cifar/blob/master/models/googlenet.py).\n",
    "\n",
    "### GoogleLeNet\n",
    "\n",
    "GoogleLeNet or Inception network is an important concept for development CNN classifier. Most of CNNs just stacked convolution deeper and deeper to get performance, but very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network, and make computation expensive. In the other hands, inception network do in wider path to improve performance.\n",
    "\n",
    "<img src=\"img/expandDeeper.jpg\" style=\"width: 400px;\" />\n",
    "\n",
    "There are several versions of the inception networks such as Inception v1, Inception v2, Inception v3, Inception v4, and Inception-ResNet.\n",
    "\n",
    "The full architecture of GoogLeNet (inception1) looks like this:\n",
    "\n",
    "<img src=\"img/GoogleLeNet.png\" style=\"width: 1080px;\" />\n",
    "\n",
    "### Inception block\n",
    "\n",
    "The key innovation introduced by GoogLeNet is the concept of the \"inception\" block. A standard inception block looks like this:\n",
    "\n",
    "<img src=\"img/inception.png\" style=\"width: 600px;\" />\n",
    "\n",
    "### Auxiliary classifiers\n",
    "\n",
    "To prevent the middle part of the network from “dying out”, the authors introduced two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. Weight value used in the paper was 0.3 for each auxiliary loss.\n",
    "\n",
    "$$ \\mathcal{L}_{total} = \\mathcal{L}_{Real} + 0.3 \\mathcal{L}_{aux_1} + 0.3 \\mathcal{L}_{aux_2}$$\n",
    "\n",
    "### Inception v1 coding\n",
    "\n",
    "Let's implement the architecture. \n",
    "Take a look at each element and see how it implements the concepts described in the paper.\n",
    "First, we begin with a `Module` for an inception block with parameters that can be customized to implement each block in the overall network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    Inception block for a GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_planes : int\n",
    "        Number of input feature maps\n",
    "    n1x1 : int\n",
    "        Number of direct 1x1 convolutions\n",
    "    n3x3red : int\n",
    "        Number of 1x1 reductions before the 3x3 convolutions\n",
    "    n3x3 : int\n",
    "        Number of 3x3 convolutions\n",
    "    n5x5red : int\n",
    "        Number of 1x1 reductions before the 5x5 convolutions\n",
    "    n5x5 : int\n",
    "        Number of 5x5 convolutions\n",
    "    pool_planes : int\n",
    "        Number of 1x1 convolutions after 3x3 max pooling\n",
    "    b1 : Sequential\n",
    "        First branch (direct 1x1 convolutions)\n",
    "    b2 : Sequential\n",
    "        Second branch (reduction then 3x3 convolutions)\n",
    "    b3 : Sequential\n",
    "        Third branch (reduction then 5x5 convolutions)\n",
    "    b4 : Sequential\n",
    "        Fourth branch (max pooling then reduction)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.n1x1 = n1x1\n",
    "        self.n3x3red = n3x3red\n",
    "        self.n3x3 = n3x3\n",
    "        self.n5x5red = n5x5red\n",
    "        self.n5x5 = n5x5\n",
    "        self.pool_planes = pool_planes\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1, y2, y3, y4], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The whole shebang\n",
    "\n",
    "Now the whole shebang.\n",
    "\n",
    "Note that kiangliu's version is intended for CIFAR-10, so it's assuming a small input image size (3x32x32). Also, there are no side classifiers.\n",
    "In the exercises, you'll convert this to the ImageNet style 224x224 input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    '''\n",
    "    GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pre_layers : Sequential\n",
    "        Initial convolutional layer\n",
    "    a3 : Inception\n",
    "        First inception block\n",
    "    b3 : Inception\n",
    "        Second inception block\n",
    "    maxpool : MaxPool2d\n",
    "        Pooling layer after second inception block\n",
    "    a4 : Inception\n",
    "        Third inception block\n",
    "    b4 : Inception\n",
    "        Fourth inception block\n",
    "    c4 : Inception\n",
    "        Fifth inception block\n",
    "    d4 : Inception\n",
    "        Sixth inception block\n",
    "    e4 : Inception\n",
    "        Seventh inception block\n",
    "    a5 : Inception\n",
    "        Eighth inception block\n",
    "    b5 : Inception\n",
    "        Ninth inception block\n",
    "    avgpool : AvgPool2d\n",
    "        Average pool layer after final inception block\n",
    "    linear : Linear\n",
    "        Fully connected layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a4(out)\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "        out = self.d4(out)\n",
    "        out = self.e4(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here are the Dataset and DataLoader objects from kiangliu. Notice the transforms may be more suitable for CIFAR-10 than the ImageNet transforms we implemented last week. But will they work as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess inputs to 3x32x32 with CIFAR-specific normalization parameters\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(36),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827 ,0.44653124), (0.24703233, 0.24348505, 0.26158768))])\n",
    "\n",
    "# Download CIFAR-10 and set up train, validation, and test datasets with new preprocess object\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root_path + 'data', train=True,\n",
    "                                            download=True, transform=preprocess)\n",
    "\n",
    "train_datset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root_path + 'data', train=False,\n",
    "                                            download=True, transform=preprocess)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "dataloaders = { 'train': train_dataloader, 'val': val_dataloader }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet = GoogLeNet().to(device)\n",
    "criterion_3 = nn.CrossEntropyLoss()\n",
    "params_to_update_3 = googlenet.parameters()\n",
    "optimizer_3 = optim.Adam(params_to_update_3, lr=0.01)\n",
    "\n",
    "best_model3, val_acc_history3, loss_acc_history3 = train_model(googlenet, dataloaders, criterion_3, optimizer_3, 25, 'googlenet_lr_0.01_bestsofar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(val_acc_history3, loss_acc_history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "694972215d13d22bf7565aec6c4075dc",
     "grade": false,
     "grade_id": "cell-c84fc25dd816d15b",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try to run the AlexNet and Inception net at above. Increase the number of epochs, research on your best hyperparameters and show your accuracy/loss. From your best settings, add the auxiliary classifier loss with the paper mentioned weights (0.3) and compare with your previous training result in GoogLeNet. Explain the result in one paragraph.\n",
    "\n",
    "2. Please take a look at VGG model architecture for ImageNet dataset as below. Create your own VGG model for CIFAR-10 (the last layer is 10 output). Train the model, show your accuracy of test set, and plot your accuracy, loss of training per epoch.\n",
    "<img src=\"img/vgg16.png\" title=\"VGG6\" style=\"width: 900px;\" />\n",
    "3. Please look at inception net model as below. All modules can release the same output. Modify the inception model in to fig 6.5. Training and show your result.  \n",
    "\n",
    "**Hint**: Just modify only inception class. Check conv2D function [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)  \n",
    "\n",
    "<img src=\"img/fig9.png\" title=\"Inception images\" style=\"width: 600px;\" />\n",
    "\n",
    "The evolution of inception is below.\n",
    "<img src=\"img/Inception v3.jpeg\" title=\"Inception evolution\" style=\"width: 900px;\" />\n",
    "\n",
    "#### You can read the paper in [GoogLeNet Paper](https://arxiv.org/abs/1512.00567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6ffee1272e5615d463e938cd2b953ff",
     "grade": false,
     "grade_id": "cell-a1d70566ff1cc75b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# VGG16 answer\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74f1200d3497567d61b41902a41445dd",
     "grade": false,
     "grade_id": "cell-efcedf10ed020e55",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Inception answer\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
