{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370fe428",
   "metadata": {},
   "source": [
    "# 05-ResNet and SENet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57cd20",
   "metadata": {},
   "source": [
    "In this lab, we will implement one of the most popular CNN architectures, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), with >237k citations and so called CNN's complementary enhancement model, [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507).\n",
    "\n",
    "ResNet model won the 1st place in ILSVRC 2015 classification challenge. The extremely deep representations also have excellent generalization performance on other recognition tasks, winning the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e6b39",
   "metadata": {},
   "source": [
    "## ResNet  \n",
    "### The Problem of Deeper Neural Networks  \n",
    "1. Vanishing/ Exploding Gradients\n",
    "2. Overfitting\n",
    "3. Model Degradation\n",
    "4. Optimization/ Convergence Problem\n",
    "5. Higher Computation Cost  \n",
    "\n",
    "<img src=\"img/degradation.png\" width=\"400px\">  \n",
    "\n",
    "**Figure 1**: Training error (left) and testing error (right) on CIFAR-10\n",
    "with 20-layer and 56-layer “plain” networks. The deeper network\n",
    "has higher training error, and thus test error.\n",
    "\n",
    "### How Degradation Problem Solved\n",
    "Introducing a deep residual learning framework, by explicitly let the layers fit a residual mapping instead of these layers directly fit a desired underlying mapping:\n",
    "\n",
    "$$\\mathcal{F}(\\text{x}):=\\mathcal{H}(\\text{x})-\\text{x}$$\n",
    "\n",
    "where:\n",
    "$$\\mathcal{F}(\\text{x}) = \\text{residual function}$$\n",
    "$$\\mathcal{H}(\\text{x}) = \\text{desired underlying mapping}$$\n",
    "$$\\text{x} = \\text{identity mapping}$$  \n",
    "\n",
    "If $\\mathcal{H}(\\text{x})=\\text{x}$, then $\\mathcal{F}(\\text{x}) = 0$, which means zero residual and identity mapping by a stack of nonlinear layers.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/residual.png\" width=\"200px\">  \n",
    "</div>\n",
    "\n",
    "**Figure 2**: Residual Learning, a building block.\n",
    "Shortcut Connections or Skip Connections:  \n",
    "\n",
    "\n",
    "$$\\text{y} = \\mathcal{F}(\\text{x}, \\{\\text{W}_{i}\\})+\\text{x}$$  \n",
    "where:  \n",
    "x, y = input, output  \n",
    "$\\mathcal{F}(\\text{x}, \\{\\text{W}_{i}\\})$ = residual mapping to be learned  \n",
    "\n",
    "$$\\mathcal{F} = \\text{W}_{2}\\sigma(\\text{W}_{1}\\text{x})$$ \n",
    "where:  \n",
    "$\\sigma$ = ReLU non-linearity  \n",
    "\n",
    "Then, $\\mathcal{F} + \\text{x}$, performs element-wise addition  \n",
    "\n",
    "The dimension of $\\text{x}$ and $\\mathcal{F}$ must be equal when performing addition operation. If this is not the case, we can perform a linear projection $\\text{W}_{s}$ by the shortcut connections to match the dimensions:  \n",
    "$$\\text{y} = \\mathcal{F}(\\text{x}, \\{\\text{W}\\}_{i}) + \\text{W}_{s}\\text{x}$$  \n",
    "\n",
    "<img src=\"img/resnet_graph.png\" alt=\"Residual Training\" width=\"500px\">\n",
    "\n",
    "**Figure 3**: Training on **ImageNet**. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34-layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts.  \n",
    "\n",
    "**Table 1**: Top-1 error(%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts.  \n",
    "\n",
    "|           | plain    | ResNet   |\n",
    "|-----------|----------|----------|\n",
    "| 18 layers | 27.94    | 27.88    |\n",
    "| 34 layers | 28.54    | **25.03**|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b66d58",
   "metadata": {},
   "source": [
    "### ResNet Structure\n",
    "\n",
    "ResNet structure has 4 stages. Each stage consists of a number of residual blocks. The number of residual blocks in each stage can be written as [$s_{1}, s_{2}, s_{3}, s_{4}$]. For example, in ResNet34, we have [3,4,6,3] number of blocks.\n",
    "\n",
    "### ResNet18\n",
    "\n",
    "ResNet18 is the simplest architecture among ResNet different models. It consists of 18 layers with 1.8 GFLOP operations per second and [2,2,2,2] residual blocks (two convolutional layers in each residual block) together with linear and softmax layers.\n",
    "\n",
    "<img src=\"img/resnet18.png\" alt=\"ResNet18\" width=\"700px\">\n",
    "\n",
    "**Figure 4**: ResNet18 Architecture. At different stages, two residual connections are added in every two convolutional layers. The input size of first convolutional layer of each stage is spatially reduced to half and increased channel-wise to two times.  \n",
    "\n",
    "### Residual Blocks\n",
    "**Basic Residual Block**  \n",
    "\n",
    "ResNet18 and ResNet34 use basic residual blocks which is the skip connection in every two convolutional layers.\n",
    "\n",
    "**Bottleneck Block**  \n",
    "\n",
    "In ResNet50 and deeper ResNet networks, a more complicated residual block, named Bottleneck Block, is used. The Bottleneck Block helps to mitigate the vanishing gradient issue in deeper layers. The components of the Bottleneck Block are described below:  \n",
    "\n",
    "- Identity shortcut connection: The identity shortcut connection is a skip connection that directly passes the input to the output of the Bottleneck Block. This helps the gradient to flow a shorter path during back propagation.\n",
    "- 1x1 convolution for dimension reduction: The first layer in Bottlenect Block is a 1x1 convolution with fewer filters than the subsequent 3x3 convolution. This reduces the dimensionality of the feature maps, making it computationally more efficient.\n",
    "- 3x3 convolution for complex feature learning: The second layer is a 3x3 convolution layer, which applies more sophisticated feature extraction to the reduced set of feature maps.\n",
    "- 1x1 convolution for feature map expansion: The final layer is another 1x1 convolutional layer that expands the number of feature maps again. This expansion allows the network to learn a richer set of features.  \n",
    "\n",
    "<img src=\"img/blocks.png\" alt=\"Basic and Bottleneck Blocks\" width=\"500px\">\n",
    "\n",
    "**Figure 5**: A deeper residual function $\\mathcal{F}$ for ImageNet. Left: a building block (on 56x56 feature maps) for ResNet-34. Right: a \"Bottleneck\" buildig block for ResNet-50/101/152.  \n",
    "\n",
    "<img src=\"img/residualblocks.png\" alt=\"Residual Blocks\" width=\"600px\">\n",
    "\n",
    "**Figure 6**: Residual Blocks. #1. Ordinary basic blocks with identity mapping #2. Residual connection transformed by 1x1 convolution to change the input feature map size. The connection is no longer identity mapping.  \n",
    "\n",
    "Let's see how to implement a residual block in a resuable way. This code is modified from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c899f",
   "metadata": {},
   "source": [
    "### Preliminaries (datasets and data loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a54843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "eval_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets.\n",
    "# The copy of the training dataset after the split allows us to keep\n",
    "# the same training/validation split of the original training set but\n",
    "# apply different transforms to the training set and validation set.\n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                  download=True)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = train_preprocess\n",
    "val_dataset.dataset.transform = eval_preprocess\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=eval_preprocess)\n",
    "\n",
    "# DataLoaders for the three datasets\n",
    "\n",
    "BATCH_SIZE=128\n",
    "NUM_WORKERS=4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5a9f5",
   "metadata": {},
   "source": [
    "### Basic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94850793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    BasicBlock: Simple residual block with two conv layers\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557e30a",
   "metadata": {},
   "source": [
    "### Bottleneck Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bde361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    '''\n",
    "    BottleneckBlock: More powerful residual block with three convs, used for Resnet50 and up\n",
    "    '''\n",
    "    EXPANSION = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.EXPANSION * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If the output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != self.EXPANSION * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.EXPANSION * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dab04",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a935d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # FC layer = 1 layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.EXPANSION, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.EXPANSION\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a06db",
   "metadata": {},
   "source": [
    "### ResNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63051ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with two sets of two convolutions each: 2*2 + 2*2 + 2*2 + 2*2 = 16 conv layers\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+16+1 = 18\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def ResNet34(num_classes):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of two convolutions each: 3*2 + 4*2 + 6*2 + 3*2 = 32\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+32+1 = 34\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet50(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of three convolutions each: 3*3 + 4*3 + 6*3 + 3*3 = 48\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+48+1 = 50\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 23, 3] sets of three convolutions each: 3*3 + 4*3 + 23*3 + 3*3 = 99\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+99+1 = 101\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 8, 36, 3] sets of three convolutions each: 3*3 + 8*3 + 36*3 + 3*3 = 150\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+150+1 = 152\n",
    "    '''\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef673331",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    '''\n",
    "    train_model: train a model on a dataset\n",
    "    \n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # for process anything, device and dataset must put in the same place.\n",
    "                # If the model is in GPU, input and output must set to GPU\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                # it uses for update training weights\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        # print('outputs', outputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4623d82",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18().to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(resnet, dataloaders, criterion, optimizer, 25, 'resnet18_bestsofar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57622bde",
   "metadata": {},
   "source": [
    "## Squeeze and Excitation Networks\n",
    "[Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "Squeeze and Excite networks (SENet) is a building block for CNNs that improves channel interdependencies at almost no computational cost. The modification from the ordinary ResNet is easy. The main idea of SENet is add parameters in each channel, then the network can adaptively adjust the weighting of each feature map.\n",
    "\n",
    "SENets are all about changing this by adding a content aware mechanism to weight each channel adaptively. In it’s most basic form this could mean adding a single parameter to each channel and giving it a linear scalar how relevant each one is.\n",
    "\n",
    "The concept of squeeze and excite (SENet) is shown here:  \n",
    "\n",
    "**Figure 7**: SE Schema, Right: SE-Inception Module, Left: SE-ResNet Module  \n",
    "<img src=\"img/senet.png\" alt=\"SENet\" width=\"600px\">  \n",
    "\n",
    "**Figure 8**: SE-ResNet  \n",
    "<img src=\"img/seresnet.png\" alt=\"SEResNet\" width=\"600px\">  \n",
    "\n",
    "**Figure 9**: SE-Inception\n",
    "<img src=\"img/seinception.png\" alt=\"SEInception\" width=\"600px\">  \n",
    "\n",
    "SE modules can be added anywhere as shown below:  \n",
    "**Figure 10**: Different Types of SENet Blocks\n",
    "<img src=\"img/senetblocks.png\" alt=\"SENet Blocks\" width=\"700px\">  \n",
    "\n",
    "Implementation is beautifully simple. Here's an example of an SE module from https://github.com/moskomule/senet.pytorch/blob/23839e07525f9f5d39982140fccc8b925fe4dee9/senet/se_module.py#L4.\n",
    "\n",
    "Let's use the standard option (option b above) recommended by the authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca57b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSEBasicBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualSEBasicBlock: Standard two-convolution residual block with an SE Module between the\n",
    "                          second convolution and the identity addition\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.se = SELayer(out_planes, reduction)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with a 1x1 conv\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)              # se net add here\n",
    "        out += self.shortcut(x)         # shortcut just plus it!!!\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResSENet18(num_classes = 10):\n",
    "    return ResNet(ResidualSEBasicBlock, [2, 2, 2, 2], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536302d",
   "metadata": {},
   "source": [
    "Let's try the SE version of ResNet18 and compare in terms of time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "ressenet = ResSENet18().to(device)\n",
    "# Optimizer, loss function\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "params_to_update2 = ressenet.parameters()\n",
    "optimizer2 = optim.Adam(params_to_update2, lr=0.01)\n",
    "\n",
    "best_model2, val_acc_history2, loss_acc_history2 = train_model(ressenet, dataloaders, criterion2, optimizer2, 25, 'ressenet18_bestsofar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(val_acc_history, loss_acc_history, val_acc_history2, loss_acc_history2):\n",
    "    plt.plot(loss_acc_history, label = 'ResNet18')\n",
    "    plt.plot(loss_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Training loss over time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(val_acc_history, label = 'ResNet18')\n",
    "    plt.plot(val_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Validation accuracy over time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(val_acc_history, loss_acc_history, val_acc_history2, loss_acc_history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bf227",
   "metadata": {},
   "source": [
    "Interestingly, we can see that the additional parameters accelerate learning of the training set without causing any degredation on the validation set and in fact improving validation set performance early on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439f219",
   "metadata": {},
   "source": [
    "### Create your own dataset\n",
    "\n",
    "If you want to use the model that you created, downloaded with your own project, you must know that each dataset does not store in the same format. You need to consider the data to get images and label as you want. For computer vision dataset, there are some example types as:\n",
    "\n",
    "1. Classification: images, labels\n",
    "    - folderClassA, folderClassB\n",
    "    - image_name\n",
    "    - images folder, csv_labels\n",
    "2. Detection: images, annotations\n",
    "    - Yolo: images folder, annotation files\n",
    "3. Segmentation: images, annotations\n",
    "    - images folder, masks folder\n",
    "    - images folder, annotation files\n",
    "4. Image synthesis: images, labels (optional)\n",
    "5. Image transfer: imagesA, imagesB\n",
    "\n",
    "In this lab, I will explain only image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f619b",
   "metadata": {},
   "source": [
    "### Experiment: Kaprao-Horapa\n",
    "\n",
    "First, let's load the [vege_dataset.zip](https://www.dropbox.com/s/eip79rx1mmofbov/vege_dataset.zip?dl=0).\n",
    "The dataset contains 2 classes of kaprao and horapa. Both are basils but different families and usages.\n",
    "\n",
    "Extract file and see the folder inside\n",
    "\n",
    "The dataset contains 2 folders with 2 different names, so we can use the folder as dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afcc6a",
   "metadata": {},
   "source": [
    "### Create Dataset class using pytorch\n",
    "\n",
    "Let's create the empty dataset class. The input of the class are\n",
    "- the dataset library of <code>../vege_dataset/</code>, when <code>..</code> is the root path of your dataset.\n",
    "- transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0773c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BasilDataset(Dataset):\n",
    "    def __init__(self, root_path=\"/vege_dataset/\", transform=None):\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1028b5",
   "metadata": {},
   "source": [
    "The important function of the dataset class are\n",
    "- <code>__init__</code>: The constructor <code>__init__</code> initializes the required parameters that are owned by the class BasilDataset.\n",
    "- <code>__len__</code>: The function returns total number of dataset\n",
    "- <code>__getitem__</code>: This function receives an index `i` as an argument which is generated from the **DataLoader** class. `i` is random if `shuffle` parameter from the **DataLoader** is set to `True`. The <code>__getitem__</code> function selects the index `i` from the dataset and perform the transforms and returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d171fe0",
   "metadata": {},
   "source": [
    "### Get one item of your dataset in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c173db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class BasilDataset(Dataset):\n",
    "    def __init__(self, root_path=\"vege_dataset/\", transform=None):\n",
    "        # keep root directory\n",
    "        self.dir = root_path\n",
    "        # keep transform\n",
    "        self.transform = transform\n",
    "\n",
    "        # read all files in kapao and horapa folder\n",
    "        list_kaprao = listdir(root_path + 'kapao/')\n",
    "        list_horapa = listdir(root_path + 'horapa/')\n",
    "        # calculate all number for each class (just in case)\n",
    "        self.kaprao_len = len(list_kaprao)\n",
    "        self.horapa_len = len(list_horapa)\n",
    "\n",
    "        # put the data file path into ids\n",
    "        self.ids = [self.dir + 'kapao/' + file for file in list_kaprao if not file.startswith('.')]\n",
    "        self.ids.extend([self.dir + 'horapa/' + file for file in list_horapa if not file.startswith('.')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.kaprao_len + self.horapa_len\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        img_file = idx\n",
    "        \n",
    "        # open photo\n",
    "        pil_img = Image.open(img_file)\n",
    "        \n",
    "        # resize, normalize and convert to pytorch tensor\n",
    "        if self.transform:\n",
    "            img = self.transform(pil_img)\n",
    "        self.pil_img = pil_img\n",
    "            \n",
    "        # get label from file list counter\n",
    "        if i < self.kaprao_len:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': label,\n",
    "            'file_name' : img_file,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5310a",
   "metadata": {},
   "source": [
    "### Test dataset\n",
    "\n",
    "Now you can test your dataset to get images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"vege_dataset/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(28), # CenterCrop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = BasilDataset(root, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_label = ['kaprao', 'horapa']\n",
    "\n",
    "batch = dataset[0]\n",
    "image, label, filename = batch['image'], batch['label'], batch['file_name']\n",
    "pil_img = Image.open(filename)\n",
    "\n",
    "print(output_label[label])\n",
    "print(filename)\n",
    "# (3, 224, 224) pytorch\n",
    "# pyplot -> (224,224,3)\n",
    "plt.imshow(pil_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975e2cc",
   "metadata": {},
   "source": [
    "### Create a Train Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56497821",
   "metadata": {},
   "source": [
    "### Initialize an instance using the resnet class we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18(2).to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "loss_history = []\n",
    "loss_history_epoch = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    for batch in train_loader:\n",
    "        image, label, filename = batch['image'], batch['label'], batch['file_name']\n",
    "            \n",
    "        epoch_iter += image.shape[0]\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # training only\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = resnet(image)\n",
    "\n",
    "        # 0, 1, 0, 0 ---> 0.2, 0.6, 0.1, 0.1\n",
    "        loss = criterion(output, label)       # training\n",
    "\n",
    "        # prediction - real use\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        running_corrects += torch.sum(preds == label.data)\n",
    "\n",
    "        loss.backward()       # back propagation -> calculate that how much value to update weight\n",
    "        optimizer.step()      #update weight\n",
    "\n",
    "        loss_history.append(loss.item() * image.size(0))\n",
    "        if (epoch_iter % 640 == 0):\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(epoch_iter, loss.item(), running_corrects / epoch_iter))\n",
    "        \n",
    "    loss_history_epoch.append(running_loss / epoch_iter)\n",
    "    accuracy.append(running_corrects / epoch_iter)\n",
    "\n",
    "    print('Epoch: {} Loss: {:.4f} Acc: {:.4f}'.format(epoch, running_loss / epoch_iter, running_corrects / epoch_iter * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ef3f1",
   "metadata": {},
   "source": [
    "## Take home exercises\n",
    "1. Run the lab instruction. For the dataset part, split randomly to the data into 90% of train set and 10% of test set. (30 points)\n",
    "2. Create InceptionResNet. Notice that 1 inception block is similar to one ResNet Module. You can use the pattern of InceptionNet from previous Lab. Train the model using CIFAR10 dataset, plot graphs on the outputs. (40 points)\n",
    "3. Find your own dataset which contains at least 3 classes. If you download from somewhere, please reference in your report. Make your own dataset class, explain how to setup your data and the label. Train the dataset in ResNet and InceptionResNet, show your results. (30 points)\n",
    "\n",
    "### Turn-in report\n",
    "\n",
    "Export the output of the lab in PDF. You can do in the same file or create separate files of your homework and in-class exercise. Submit in PDF file and Jupyter notebook.\n",
    "\n",
    "You don't need to upload dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
